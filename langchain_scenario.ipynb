{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vEvN-jTWudFG",
        "4460tn4FztxC",
        "Gl-DctuyzjNU",
        "YdBsM3TpmYS6",
        "P2Mlprm-0WQt"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit PyPDF2 langchain sentence_transformers faiss-cpu accelerate bitsandbytes gdown"
      ],
      "metadata": {
        "id": "-8bT6PRXqFKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder '1VTPGDVWpjgIt96K1gfe5MKUV-8ekW9cx' --output /content/"
      ],
      "metadata": {
        "id": "r50JRFd9cleY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for futher information you can see link below:\n",
        "1. https://colab.research.google.com/drive/12ADneM3JbJVKYTVWUqjGuYqlYs8mCnZ2?usp=sharing\n",
        "2. https://docs.google.com/presentation/d/1sefa-ettZEsD6N6kOnp4AZx3ErVOrD4tqajCyuUPdTU/edit?usp=sharing\n",
        "3. https://www.overleaf.com/read/bpbcxnszhjbk"
      ],
      "metadata": {
        "id": "0OXUXNnxVr0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 1\n",
        "We are several steps, there are:\n",
        "1. Reading the pdf files using pdfreader\n",
        "2. Langchain llms based on OpenAI and HuggingFaceHub\n",
        "3. Splitting characters data using text splitter\n",
        "5. do running chain based on docs and query on both models (OpenAI and HuggingFaceHub).\n",
        "\n",
        "\n",
        "*if you want to use openAI key, you should checkout [this link](https://platform.openai.com/account/api-keys) and don't forget to create new openai token"
      ],
      "metadata": {
        "id": "vEvN-jTWudFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config_scenario1.json\n",
        "{\n",
        "  \"pdf_param\": \"/content/paper_target/CLOUDITY_Cloud_Supply_Chain_Framework_Design_based_on_JUGO_and_Blockchain.pdf\",\n",
        "  \"save_param\": \"/content/paper_target/data.json\",\n",
        "  \"huggingface_token_param\": \"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\",\n",
        "  \"openapi_token_param\": \"your open api key\",\n",
        "  \"chunk_size_param\": 1000,\n",
        "  \"chunk_overlap_param\": 200,\n",
        "  \"huggingface_active\": faiss,\n",
        "  \"openai_active\": true,\n",
        "  \"queries\": [\n",
        "    \"who are the authors of the article?\",\n",
        "    \"What is the title from this article?\",\n",
        "    \"What are Theoretical/ Conceptual Framework from this article?\",\n",
        "    \"What are Research Question(s)/ Hypotheses from this article?\",\n",
        "    \"How is the methodology works from this article?\",\n",
        "    \"What is Analysis & Results study from this article?\",\n",
        "    \"What is conclusion from this research?\",\n",
        "    \"What is Implications for Future research from this research?\",\n",
        "    \"What is Implication for practice from this research?\",\n",
        "    \"How many relevancy to the research topic from that article From 1 to 10 ?\",\n",
        "    \"What is inclusion in that article?\",\n",
        "    \"What is justification (if applicable) from that article?\"\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "S_3fC8-HKHCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d09d6ca-c203-49cf-9ea4-2cfbc3b3a03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config_scenario1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/scenario1.py\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "def main():\n",
        "\n",
        "    # set string parameters\n",
        "    parser = argparse.ArgumentParser(description='Final Project: Scenario1')\n",
        "    parser.add_argument('--config_path', type=str, default='config_scenario1.json', help='path to the JSON config file')\n",
        "\n",
        "    # variable parser explanation\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config_path = args.config_path\n",
        "    config = load_config(config_path)\n",
        "\n",
        "    pdf_param = config['pdf_param']\n",
        "    huggingface_token_param = config['huggingface_token_param']\n",
        "    openapi_token_param = config['openapi_token_param']\n",
        "    save_param = config['save_param']\n",
        "    chunk_size_param = config['chunk_size_param']\n",
        "    chunk_overlap_param = config['chunk_overlap_param']\n",
        "    queries = config['queries']\n",
        "\n",
        "    # location of the pdf file/files\n",
        "    doc_reader = PdfReader(pdf_param)\n",
        "\n",
        "    # read data from the file and put them into a variable called raw_text\n",
        "    raw_text = ''\n",
        "    for i, page in enumerate(doc_reader.pages):\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            raw_text += text\n",
        "\n",
        "    print(raw_text[:100])\n",
        "\n",
        "    # Splitting up the text into smaller chunks for indexing\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size= chunk_size_param,\n",
        "        chunk_overlap= chunk_overlap_param,  # striding over the text\n",
        "        length_function=len,\n",
        "    )\n",
        "    texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "    # Download embeddings from OpenAI\n",
        "    if huggingface_active == True:\n",
        "        os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_token_param\n",
        "        embeddings= HuggingFaceEmbeddings()\n",
        "\n",
        "    if openai_active == True:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openapi_token_param\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    docsearch = embeddings.from_texts(texts) # Indexing process without FAISS\n",
        "    responses = []\n",
        "\n",
        "    for query in queries:\n",
        "        docs = docsearch.similarity_search(query)\n",
        "        response = chain.run(input_documents=docs, question=query)\n",
        "        responses.append(response)\n",
        "        print(response)\n",
        "\n",
        "    # Sample string\n",
        "    data = {\n",
        "      \"author\": response[0],\n",
        "      \"title\": response[1],\n",
        "      \"Theoretical/ Conceptual Framework\": response[2],\n",
        "      \"Research Question(s)/ Hypotheses\": response[3],\n",
        "      \"methodology\": response[4],\n",
        "      \"Analysis & Results study\": response[5],\n",
        "      \"conclusion\": response[6],\n",
        "      \"Implications for Future research\": response[7],\n",
        "      \"Implication for practice\": response[8],\n",
        "    }\n",
        "\n",
        "    # Save the JSON object to a file\n",
        "    with open(save_param, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()"
      ],
      "metadata": {
        "id": "0NuzxLkJs4RV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027b2f0f-cff9-4eeb-e6fa-0c907b255fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/scenario1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 2\n",
        "We are several steps, there are:\n",
        "1. Reading the pdf files using pdfreader\n",
        "2. Langchain llms based on OpenAI and HuggingFaceHub\n",
        "3. Splitting characters data using text splitter\n",
        "4. do vectorstores (optional)\n",
        "    - ElasticVectorSearch\n",
        "    - Pinecone\n",
        "    - Weaviate\n",
        "    - FAISS\n",
        "5. do load_qa_chain based on query for chaining quetion and answering from both models (OpenAI and. HuggingFaceHub).\n",
        "\n",
        "*if you want to use openAI key, you should checkout [this link](https://platform.openai.com/account/api-keys) and don't forget to create new openai token"
      ],
      "metadata": {
        "id": "4460tn4FztxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config_scenario2.json\n",
        "{\n",
        "  \"data_dir_param\": \"/content/paper_target/\",\n",
        "  \"save_param\": \"/content/paper_target/data.json\",\n",
        "  \"repo_id_param\": \"MBZUAI/LaMini-Neo-1.3B\",\n",
        "  \"huggingface_token_param\": \"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\",\n",
        "  \"openapi_token_param\": \"your open api key\",\n",
        "  \"openapi_model_param\": \"text-davinci-003\",\n",
        "  \"chain_type_param\": \"stuff\",\n",
        "  \"device_param\": \"cuda\",\n",
        "  \"temperature_param\": 0.7,\n",
        "  \"max_length_param\": 500,\n",
        "  \"pad_token_id_param\": 50256,\n",
        "  \"top_p_param\": 0.95,\n",
        "  \"repetation_penalty_param\": 1.15,\n",
        "  \"chunk_size_param\": 1000,\n",
        "  \"chunk_overlap_param\": 200,\n",
        "  \"huggingface_active\": true,\n",
        "  \"openapi_active\": false,\n",
        "  \"queries\": [\n",
        "    \"who are the authors of the article?\",\n",
        "    \"What is the title from this article?\",\n",
        "    \"What are Theoretical/ Conceptual Framework from this article?\",\n",
        "    \"What are Research Question(s)/ Hypotheses from this article?\",\n",
        "    \"How is the methodology works from this article?\",\n",
        "    \"What is Analysis & Results study from this article?\",\n",
        "    \"What is conclusion from this research?\",\n",
        "    \"What is Implications for Future research from this research?\",\n",
        "    \"What is Implication for practice from this research?\",\n",
        "    \"How many relevancy to the research topic from that article From 1 to 10 ?\",\n",
        "    \"What is inclusion in that article?\",\n",
        "    \"What is justification (if applicable) from that article?\"\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "PVKdl8XtK0oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c1f826-9d42-4eef-ab04-3d2ba3665d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config_scenario2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/scenario2.py\n",
        "import os, json, argparse, torch\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import  FAISS #, ElasticVectorSearch, Pinecone, Weaviate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI, HuggingFaceHub\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Final Project: scenario2')\n",
        "    parser.add_argument('--config_path', type=str, default='config_scenario2.json', help='path to the config.json file')\n",
        "\n",
        "    # variable parser explanation\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config_path = args.config_path\n",
        "    config = load_config(config_path)\n",
        "\n",
        "    # Extract parameters from the config dictionary\n",
        "    data_dir_param = config['data_dir_param']\n",
        "    save_param = config['save_param']\n",
        "    repo_id_param = config['repo_id_param']\n",
        "    huggingface_token_param = config['huggingface_token_param']\n",
        "    openapi_token_param = config['openapi_token_param']\n",
        "    openapi_model_param = config['openapi_model_param']\n",
        "    chain_type_param = config['chain_type_param']\n",
        "    device_param = config['device_param']\n",
        "    temperature_param = config['temperature_param']\n",
        "    max_length_param = config['max_length_param']\n",
        "    pad_token_id_param = config['pad_token_id_param']\n",
        "    top_p_param = config['top_p_param']\n",
        "    repetation_penalty_param = config['repetation_penalty_param']\n",
        "    chunk_size_param = config['chunk_size_param']\n",
        "    chunk_overlap_param = config['chunk_overlap_param']\n",
        "    huggingface_active = config['huggingface_active']\n",
        "    openapi_active = config['openapi_active']\n",
        "    queries = config['queries']\n",
        "\n",
        "    # load data directory\n",
        "    data_dir = data_dir_param\n",
        "    data_list = os.listdir(data_dir)\n",
        "\n",
        "    data_article= []\n",
        "    for file_name in data_list:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
        "            article_data = f.read()\n",
        "            data_article.append(article_data)\n",
        "\n",
        "    # location of the pdf file/files.\n",
        "    for num in data_list:\n",
        "        reader = PdfReader(os.path.join(data_dir, num))\n",
        "        # read data from the file and put them into a variable called raw_text\n",
        "        raw_text = ''\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                raw_text += text\n",
        "\n",
        "        # We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits.\n",
        "        text_splitter = CharacterTextSplitter(\n",
        "            separator = \"\\n\",\n",
        "            chunk_size = chunk_size_param,\n",
        "            chunk_overlap  = chunk_overlap_param,\n",
        "            length_function = len,\n",
        "        )\n",
        "        texts = text_splitter.split_text(raw_text)\n",
        "        print(len(texts))\n",
        "\n",
        "        # Download embeddings from OpenAI\n",
        "        if huggingface_active == True:\n",
        "            os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_token_param\n",
        "            embeddings= HuggingFaceEmbeddings()\n",
        "\n",
        "            llm_hf = HuggingFaceHub(\n",
        "                      repo_id= repo_id_param,\n",
        "                      model_kwargs= {'temperature': temperature_param,\n",
        "                                      'max_length': max_length_param,\n",
        "                                      'pad_token_id': pad_token_id_param,\n",
        "                                      'top_p': top_p_param,\n",
        "                                      'device': device_param,\n",
        "                                      'repetition_penalty': repetation_penalty_param}\n",
        "            )\n",
        "\n",
        "            chain = load_qa_chain(llm= llm_hf, chain_type= chain_type_param)\n",
        "\n",
        "        if openapi_active == True:\n",
        "            os.environ[\"OPENAI_API_KEY\"] = openapi_token_param\n",
        "            embeddings = OpenAIEmbeddings()\n",
        "            chain = load_qa_chain(llm= OpenAI(model= openapi_model_param), chain_type= chain_type_param)\n",
        "\n",
        "        docsearch = FAISS.from_texts(texts, embeddings)\n",
        "        responses = []\n",
        "\n",
        "        for query in queries:\n",
        "            docs = docsearch.similarity_search(query)\n",
        "            #response = chain.run(input_documents=docs, question=query, parameters={'truncation': 'only_first'})\n",
        "            response = chain.run(input_documents=docs, question=query)\n",
        "            responses.append(response)\n",
        "            print(response)\n",
        "\n",
        "        # Sample string\n",
        "        data = {\n",
        "          \"author\": response[0],\n",
        "          \"title\": response[1],\n",
        "          \"Theoretical/ Conceptual Framework\": response[2],\n",
        "          \"Research Question(s)/ Hypotheses\": response[3],\n",
        "          \"methodology\": response[4],\n",
        "          \"Analysis & Results study\": response[5],\n",
        "          \"conclusion\": response[6],\n",
        "          \"Implications for Future research\": response[7],\n",
        "          \"Implication for practice\": response[8],\n",
        "        }\n",
        "\n",
        "        # Save the JSON object to a file\n",
        "        with open(save_param, 'w') as f:\n",
        "            json.dump(data, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n"
      ],
      "metadata": {
        "id": "v6fRPv16wEVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c9ce2e-062c-4aa1-d97a-c787d82d64a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/scenario2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approach\n",
        "We are several steps, there are:\n",
        "1. Load multiple files from directory\n",
        "2. Divide and conquer using text splitter\n",
        "3. Get embedding based on our documents\n",
        "    - Huggingface Instructor Embeddings\n",
        "    - Open AI's Embeddings\n",
        "4. Testing Both Models\n",
        "\n",
        "*if you want to use openAI key, you should checkout [this link](https://platform.openai.com/account/api-keys) and don't forget to create new openai token\n"
      ],
      "metadata": {
        "id": "Gl-DctuyzjNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile approach.json\n",
        "{\n",
        "  \"pdf_param\": \"/content/paper_target/paper1.pdf\",\n",
        "  \"folder_param\": \"/content/paper_target\",\n",
        "  \"db_param\": \"/content/paper_target/db\",\n",
        "  \"save_param\": \"/content/paper_target/data.json\",\n",
        "\n",
        "  \"checkpoint_loader_param\": \"MBZUAI/LaMini-Neo-1.3B\",\n",
        "  \"model_name_param\": \"hkunlp/instructor-xl\",\n",
        "  \"chain_type_param\": \"stuff\",\n",
        "  \"device_param\": \"cuda\",\n",
        "  \"huggingface_token_param\": \"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\",\n",
        "  \"openapi_token_param\": \"your open api key\",\n",
        "  \"method_param\": \"chroma_instructor\",\n",
        "  \"max_length_param\": 512,\n",
        "  \"pad_token_id_param\": 50256,\n",
        "  \"temperature_param\": 0.7,\n",
        "  \"top_p_param\": 0.95,\n",
        "  \"repetation_penalty_param\": 1.15,\n",
        "  \"k_param\": 3,\n",
        "  \"chunk_size_param\": 1000,\n",
        "  \"chunk_overlap_param\": 1000,\n",
        "  \"load_one_file_active\": true,\n",
        "  \"load_one_folder_active\": false,\n",
        "  \"local_llm_active\": false,\n",
        "  \"openapi_llm_active\": true,\n",
        "\n",
        "  \"queries\": [\n",
        "    \"who are the authors of the article?\",\n",
        "    \"What is the title from this article?\",\n",
        "    \"What are Theoretical/ Conceptual Framework from this article?\",\n",
        "    \"What are Research Question(s)/ Hypotheses from this article?\",\n",
        "    \"How is the methodology works from this article?\",\n",
        "    \"What is Analysis & Results study from this article?\",\n",
        "    \"What is conclusion from this research?\",\n",
        "    \"What is Implications for Future research from this research?\",\n",
        "    \"What is Implication for practice from this research?\"\n",
        "    \"How many relevancy to the research topic from that article From 1 to 10 ?\",\n",
        "    \"What is inclusion in that article?\",\n",
        "    \"What is justification (if applicable) from that article?\"\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "a33mkKcjNhB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2936b777-c417-4c67-93c1-71b49ec10a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing approach.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBytAKVkUet6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8b671f-c8b4-45a7-8487-569ad67e77e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing approach.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile approach.py\n",
        "import torch, transformers, os, json, textwrap, pickle, faiss, textwrap\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "def store_embeddings(docs, embeddings, store_name, path):\n",
        "    vectorStore = FAISS.from_documents(docs, embeddings)\n",
        "    with open(f\"{path}/faiss_{store_name}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(vectorStore, f)\n",
        "\n",
        "def load_embeddings(store_name, path):\n",
        "    with open(f\"{path}/faiss_{store_name}.pkl\", \"rb\") as f:\n",
        "        VectorStore = pickle.load(f)\n",
        "    return VectorStore\n",
        "\n",
        "def get_prompt(human_prompt):\n",
        "    prompt_template=f\"### Human: {human_prompt} \\n### Assistant:\"\n",
        "    return prompt_template\n",
        "\n",
        "def remove_human_text(text):\n",
        "    return text.split('### Human:', 1)[0]\n",
        "\n",
        "def parse_text(data):\n",
        "    for item in data:\n",
        "        text = item['generated_text']\n",
        "        assistant_text_index = text.find('### Assistant:')\n",
        "        if assistant_text_index != -1:\n",
        "            assistant_text = text[assistant_text_index+len('### Assistant:'):].strip()\n",
        "            assistant_text = remove_human_text(assistant_text)\n",
        "            wrapped_text = textwrap.fill(assistant_text, width=100)\n",
        "            print(wrapped_text)\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Final Project: Approach')\n",
        "    parser.add_argument('--config_path', type=str, default='config_scenario2.json', help='path to the config.json file')\n",
        "\n",
        "    # variable parser explanation\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config_path = args.config_path\n",
        "    config = load_config(config_path)\n",
        "\n",
        "    # Extract parameters from the config dictionary\n",
        "    pdf_param = config['pdf_param']\n",
        "    folder_param = config['folder_param']\n",
        "    db_param = config['db_param']\n",
        "    save_param = config['save_param']\n",
        "    checkpoint_loader_param = config['checkpoint_loader_param']\n",
        "    model_name_param = config['model_name_param']\n",
        "    chain_type_param = config['chain_type_param']\n",
        "    device_param = config['device_param']\n",
        "    huggingface_token_param = config['huggingface_token_param']\n",
        "    openapi_token_param = config['openapi_token_param']\n",
        "    max_length_param = config['max_length_param']\n",
        "    pad_token_id_param = config['pad_token_id_param']\n",
        "    temperature_param = config['temperature_param']\n",
        "    top_p_param = config['top_p_param']\n",
        "    repetation_penalty_param = config['repetation_penalty_param']\n",
        "    k_param = config['k_param']\n",
        "    chunk_size_param = config['chunk_size_param']\n",
        "    chunk_overlap_param = config['chunk_overlap_param']\n",
        "    queries = config['queries']\n",
        "\n",
        "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_token_param\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openapi_token_param\n",
        "\n",
        "    # Load and process the text files\n",
        "    if load_one_file_active == True:\n",
        "       loader = TextLoader(pdf_param)\n",
        "    if load_one_folder_active == True:\n",
        "       loader = DirectoryLoader(folder_param, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    print(\"length of Documents: {}\".format(len(documents)))\n",
        "\n",
        "    #splitting the text into\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= chunk_size_param, chunk_overlap= chunk_overlap_param)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(\"length texts: {}\".format(len(texts)))\n",
        "\n",
        "    instructor_embeddings = HuggingFaceInstructEmbeddings(model_name= model_name_param,\n",
        "                                                          model_kwargs={\"device\": device_param})\n",
        "\n",
        "    if local_llm_active == True:\n",
        "      tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "      base_model = AutoModelForCausalLM.from_pretrained(checkpoint_loader_param,\n",
        "                                                        device_map='auto',\n",
        "                                                        torch_dtype=torch.float16,\n",
        "                                                        load_in_8bit=True)\n",
        "\n",
        "      pipe = pipeline('text-generation',\n",
        "                      model = base_model,\n",
        "                      tokenizer = tokenizer,\n",
        "                      max_length= max_length_param,\n",
        "                      do_sample=True,\n",
        "                      pad_token_id= pad_token_id_param,\n",
        "                      temperature= temperature_param,\n",
        "                      top_p= top_p_param,\n",
        "                      repetition_penalty= repetation_penalty_param\n",
        "                      )\n",
        "\n",
        "      local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "      print(local_llm(queries[0][1]))\n",
        "\n",
        "    if openapi_llm_active == True:\n",
        "      local_llm = OpenAI(temperature= temperature_param,)\n",
        "      print(local_llm(queries[0][1]))\n",
        "\n",
        "\n",
        "    if method_param == 'faiss_instructor':\n",
        "        ##method 1: using faiss embedding store\n",
        "        Embedding_store_path = folder_param\n",
        "        store_embeddings(texts,\n",
        "                    instructor_embeddings,\n",
        "                    store_name='instructEmbeddings',\n",
        "                    path=Embedding_store_path)\n",
        "\n",
        "        db_instructEmbedd = load_embeddings(store_name='instructEmbeddings',\n",
        "                                        path=Embedding_store_path)\n",
        "\n",
        "        retriever = db_instructEmbedd.as_retriever(search_kwargs={\"k\": k_param})\n",
        "\n",
        "        # create the chain to answer questions\n",
        "        qa_chain_instruction = RetrievalQA.from_chain_type(llm=OpenAI(temperature= temperature_param,),\n",
        "                                                          chain_type= chain_type_param,\n",
        "                                                          retriever=retriever,\n",
        "                                                          return_source_documents=True)\n",
        "\n",
        "\n",
        "    elif method_param == 'chroma_instructor':\n",
        "       ## Embed and store the texts\n",
        "       persist_directory = db_param ## Supplying a persist_directory will store the embeddings on disk\n",
        "       ##method 2 {secodn scenario}:\n",
        "       vectordb = Chroma.from_documents(documents=texts,\n",
        "                                        embedding=instructor_embeddings,\n",
        "                                        persist_directory=persist_directory)\n",
        "\n",
        "       retriever = vectordb.as_retriever(search_kwargs={\"k\": k_param})\n",
        "\n",
        "       # create the chain to answer questions\n",
        "       qa_chain_instruction = RetrievalQA.from_chain_type(llm=OpenAI(temperature= temperature_param,),\n",
        "                                                        chain_type= chain_type_param,\n",
        "                                                        retriever=retriever,\n",
        "                                                        return_source_documents=True)\n",
        "\n",
        "    #example\n",
        "    #docs = retriever.get_relevant_documents(queries[0][1])\n",
        "    #print(docs[0])\n",
        "\n",
        "    elif method_param == 'openai':\n",
        "      #method 3 {third scenario}:\n",
        "      store_embeddings(texts,\n",
        "                       embeddings= OpenAIEmbeddings(),\n",
        "                       store_name= 'openAIEmbeddings',\n",
        "                       path=Embedding_store_path)\n",
        "\n",
        "      db_openAIEmbedd = load_embeddings(store_name='openAIEmbeddings',\n",
        "                                         path=Embedding_store_path)\n",
        "\n",
        "      retriever_openai = db_openAIEmbedd.as_retriever(search_kwargs={\"k\": k_param})\n",
        "\n",
        "      # create the chain to answer questions\n",
        "      qa_chain_openai = RetrievalQA.from_chain_type(llm=OpenAI(temperature= temperature_param, ),\n",
        "                                                    chain_type= chain_type_param,\n",
        "                                                    retriever= retriever_openai,\n",
        "                                                    return_source_documents=True)\n",
        "\n",
        "    # print(get_prompt(queries[0][1]))\n",
        "\n",
        "    # data = [{'generated_text': '### Human: What is the capital of England? \\n### Assistant: The capital city of England is London.'}]\n",
        "    # parse_text(data)\n",
        "\n",
        "    # qa_chain.retriever.search_type , qa_chain.retriever.vectorstore\n",
        "    # print(qa_chain.combine_documents_chain.llm_chain.prompt.template)\n",
        "\n",
        "    # qa_chain.combine_documents_chain.llm_chain.prompt.template ='''### Human: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    # {context}\n",
        "    # Question:  {question}\n",
        "    # \\n### Assistant:'''\n",
        "\n",
        "    responses = []\n",
        "\n",
        "    for query in queries:\n",
        "        print('-------------------Instructor Embeddings------------------\\n')\n",
        "        if method2 == instruction_chain:\n",
        "          llm_response = qa_chain_instruction(query)\n",
        "        if method2 == openai_chain:\n",
        "          llm_response = qa_chain_openai(query)\n",
        "\n",
        "        response = process_llm_response(llm_response)\n",
        "        responses.append(response)\n",
        "        print(response)\n",
        "\n",
        "    # Sample string\n",
        "    data = {\n",
        "      \"author\": response[0],\n",
        "      \"title\": response[1],\n",
        "      \"Theoretical/ Conceptual Framework\": response[2],\n",
        "      \"Research Question(s)/ Hypotheses\": response[3],\n",
        "      \"methodology\": response[4],\n",
        "      \"Analysis & Results study\": response[5],\n",
        "      \"conclusion\": response[6],\n",
        "      \"Implications for Future research\": response[7],\n",
        "      \"Implication for practice\": response[8],\n",
        "    }\n",
        "\n",
        "    # Save the JSON object to a file\n",
        "    with open(save_param, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Table"
      ],
      "metadata": {
        "id": "YdBsM3TpmYS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install similarity"
      ],
      "metadata": {
        "id": "O5HbvRDAouPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932b04c4-69ff-448d-b267-6e92e7c994dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting similarity\n",
            "  Downloading similarity-0.0.1-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from similarity) (0.6.2)\n",
            "Collecting jellyfish (from similarity)\n",
            "  Downloading jellyfish-0.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from similarity) (1.22.4)\n",
            "Collecting interaction (from similarity)\n",
            "  Downloading interaction-1.3-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: jellyfish, interaction, similarity\n",
            "Successfully installed interaction-1.3 jellyfish-0.11.2 similarity-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile evaluation_onefeature.py\n",
        "#@title Evaluation Expert Data One Feature\n",
        "evaluation_data = '/content/paper_target/SLR Assessment.xlsx' #@param {type:\"raw\"}\n",
        "# variable_data = \"Implication for Future Research\" #@param [\"Title\", \"Abstract\", \"Introduction\", \"Methods\", \"Results\", \"Discussion/Conclusion\", \"Theoritical/ Conceptual Framework\", \"Implication for Future Research\", \"Implication for Practice from this Research\"]\n",
        "variable_data = \"Abstract\" #@param [\"Title\", \"Abstract\", \"Introduction\", \"Methods\", \"Results\", \"Discussion/Conclusion\", \"Theoritical/ Conceptual Framework\", \"Implication for Future Research\", \"Implication for Practice from this Research\"]\n",
        "mod_data = \"Justification\" #@param [\"Justification\", \"Relevancy\", \"Inclusion\"]\n",
        "custom_threshold = 0.55 #@param {type:\"number\"}\n",
        "import similarity, pandas as pd\n",
        "\n",
        "# Define a custom style function\n",
        "def highlight_below_standard(value):\n",
        "    if value < custom_threshold:\n",
        "        return 'background-color: red'\n",
        "    elif value >= custom_threshold and value < custom_threshold:\n",
        "        return 'background-color: yellow'\n",
        "    else:\n",
        "        return 'background-color: green'\n",
        "\n",
        "df_response = pd.read_excel(evaluation_data, sheet_name='response')# Read the Excel file into a pandas DataFrame based on sheet\n",
        "df_system = pd.read_excel(evaluation_data, sheet_name='system')# Read the Excel file into a pandas DataFrame based on sheet\n",
        "\n",
        "data1_paper1 = df_system['Study {} {} of Paper 1 Software Requirement Engineering'.format(variable_data, mod_data)]\n",
        "data2_paper1 = df_response['Study {} {} of Paper 1 Software Requirement Engineering'.format(variable_data, mod_data)]\n",
        "data1_paper2 = df_system['Study {} {} of Paper 2 Software Quality Assurance'.format(variable_data, mod_data)]\n",
        "data2_paper2 = df_response['Study {} {} of Paper 2 Software Quality Assurance'.format(variable_data, mod_data)]\n",
        "data1_paper3 = df_system['Study {} {} of Paper 3 Network-based Computing'.format(variable_data, mod_data)]\n",
        "data2_paper3 = df_response['Study {} {} of Paper 3 Network-based Computing'.format(variable_data, mod_data)]\n",
        "data1_paper4 = df_system['Study {} {} of Paper 4 Visual Image Computation'.format(variable_data, mod_data)]\n",
        "data2_paper4 = df_response['Study {} {} of Paper 4 Visual Image Computation'.format(variable_data, mod_data)]\n",
        "data1_paper5 = df_system['Study {} {} of Paper 5 Visual Image Computation'.format(variable_data, mod_data)]\n",
        "data2_paper5 = df_response['Study {} {} of Paper 5 Visual Image Computation'.format(variable_data, mod_data)]\n",
        "data_paper1 = [[similarity.get_string_similarity(num, angka) for angka in data2_paper1] for num in data1_paper1][0]\n",
        "data_paper2 = [[similarity.get_string_similarity(num, angka) for angka in data2_paper2] for num in data1_paper2][0]\n",
        "data_paper3 = [[similarity.get_string_similarity(num, angka) for angka in data2_paper3] for num in data1_paper3][0]\n",
        "data_paper4 = [[similarity.get_string_similarity(num, angka) for angka in data2_paper4] for num in data1_paper4][0]\n",
        "data_paper5 = [[similarity.get_string_similarity(num, angka) for angka in data2_paper5] for num in data1_paper5][0]\n",
        "\n",
        "print(\"\\nData System\")\n",
        "print(data1_paper1)\n",
        "print(\"\\nData Respondend\")\n",
        "print(data2_paper1)\n",
        "print(\"\\nMatrix Evaluation\")\n",
        "print(data_paper1)\n",
        "\n",
        "df_sim = pd.DataFrame([data_paper1, data_paper2, data_paper3, data_paper4, data_paper5])\n",
        "print(\"Evaluation Study {} {} w/ one feature\".format(variable_data, mod_data))\n",
        "# df_sim.columns = [num for id, num in enumerate(df_response.Name)]\n",
        "df_sim.columns = [\"author {}\".format(id+1) for id, num in enumerate(df_sim.columns)]\n",
        "df_sim.index = [\"article {}\".format(id+1) for id, num in enumerate(df_sim.index)]\n",
        "df_sim\n",
        "# df_sim.T.describe()\n",
        "# df_sim.T.max()\n",
        "\n",
        "# Apply the style function to the DataFrame\n",
        "# styled_df = df_sim.style.applymap(highlight_below_standard, subset=['Score'])\n",
        "# styled_df = df_sim.style.applymap(highlight_below_standard)\n",
        "# print(styled_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "tsIFTcg-oyo5",
        "outputId": "e402ac16-17d7-4674-95c4-3cf49601467c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data System\n",
            "0    This section provides an overview of the prima...\n",
            "Name: Study Abstract Justification of Paper 1 Software Requirement Engineering, dtype: object\n",
            "\n",
            "Data Respondend\n",
            "0         abstract has fulfilled scientific principles\n",
            "1    clearly and concisely contains an abstract com...\n",
            "2                                              correct\n",
            "3           Study Abstract is very good and applicable\n",
            "4    It is concise and shows high clarity and accur...\n",
            "5                                          applicable \n",
            "6    After read the abstract I know some important ...\n",
            "7                                                   no\n",
            "8    It is better to provide a clear drawback and i...\n",
            "Name: Study Abstract Justification of Paper 1 Software Requirement Engineering, dtype: object\n",
            "\n",
            "Matrix Evaluation\n",
            "[0.5367519078859285, 0.5319379932781995, 0.5272459499263623, 0.5162561713982382, 0.5385052805155898, 0.5205516133351185, 0.5654147434105533, 0.5009818360333824, 0.5368550835555532]\n",
            "Evaluation Study Abstract Justification w/ one feature\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           author 1  author 2  author 3  author 4  author 5  author 6  \\\n",
              "article 1  0.536752  0.531938  0.527246  0.516256  0.538505  0.520552   \n",
              "article 2  0.556195  0.500729  0.523175  0.546508  0.545967  0.540627   \n",
              "article 3  0.523700  0.667325  0.551566  0.547528  0.543588  0.539921   \n",
              "article 4  0.535647  0.000000  0.521758  0.519017  0.500099  0.536933   \n",
              "article 5  0.515576  0.667406  0.523282  0.522024  0.472000  0.507391   \n",
              "\n",
              "           author 7  author 8  author 9  \n",
              "article 1  0.565415  0.500982  0.536855  \n",
              "article 2  0.572220  0.557744  0.543831  \n",
              "article 3  0.561395  0.557532  0.588116  \n",
              "article 4  0.564167  0.556635  0.556451  \n",
              "article 5  0.534779  0.668145  0.529758  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7c45988-1837-4526-af53-b269432b168d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author 1</th>\n",
              "      <th>author 2</th>\n",
              "      <th>author 3</th>\n",
              "      <th>author 4</th>\n",
              "      <th>author 5</th>\n",
              "      <th>author 6</th>\n",
              "      <th>author 7</th>\n",
              "      <th>author 8</th>\n",
              "      <th>author 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>article 1</th>\n",
              "      <td>0.536752</td>\n",
              "      <td>0.531938</td>\n",
              "      <td>0.527246</td>\n",
              "      <td>0.516256</td>\n",
              "      <td>0.538505</td>\n",
              "      <td>0.520552</td>\n",
              "      <td>0.565415</td>\n",
              "      <td>0.500982</td>\n",
              "      <td>0.536855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 2</th>\n",
              "      <td>0.556195</td>\n",
              "      <td>0.500729</td>\n",
              "      <td>0.523175</td>\n",
              "      <td>0.546508</td>\n",
              "      <td>0.545967</td>\n",
              "      <td>0.540627</td>\n",
              "      <td>0.572220</td>\n",
              "      <td>0.557744</td>\n",
              "      <td>0.543831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 3</th>\n",
              "      <td>0.523700</td>\n",
              "      <td>0.667325</td>\n",
              "      <td>0.551566</td>\n",
              "      <td>0.547528</td>\n",
              "      <td>0.543588</td>\n",
              "      <td>0.539921</td>\n",
              "      <td>0.561395</td>\n",
              "      <td>0.557532</td>\n",
              "      <td>0.588116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 4</th>\n",
              "      <td>0.535647</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.521758</td>\n",
              "      <td>0.519017</td>\n",
              "      <td>0.500099</td>\n",
              "      <td>0.536933</td>\n",
              "      <td>0.564167</td>\n",
              "      <td>0.556635</td>\n",
              "      <td>0.556451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 5</th>\n",
              "      <td>0.515576</td>\n",
              "      <td>0.667406</td>\n",
              "      <td>0.523282</td>\n",
              "      <td>0.522024</td>\n",
              "      <td>0.472000</td>\n",
              "      <td>0.507391</td>\n",
              "      <td>0.534779</td>\n",
              "      <td>0.668145</td>\n",
              "      <td>0.529758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7c45988-1837-4526-af53-b269432b168d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7c45988-1837-4526-af53-b269432b168d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7c45988-1837-4526-af53-b269432b168d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "al3FLHpo8pl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# /content/gdrive/MyDrive/2ndSemester/wimu_assignment/final_project/paper_target/SLR Assessment.xlsx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufgu-Wz5XuNl",
        "outputId": "ed9b09da-c47d-48d0-ea09-6ac23b97fda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile evaluation_moreFeatures.py\n",
        "#@title Evaluation Expert Data More Features\n",
        "evaluation_data = '/content/paper_target/SLR Assessment.xlsx' #@param {type:\"string\"}\n",
        "variable_data = \"Abstract\" #@param [\"Title\", \"Abstract\", \"Introduction\", \"Methods\", \"Results\", \"Discussion/Conclusion\", \"Theoritical/ Conceptual Framework\", \"Implication for Future Research\", \"Implication for Practice from this Research\"]\n",
        "# condition_data = \"default\" #@param [\"default\", \"maximum\", \"threshold\"]\n",
        "custom_threshold = 0.65 #@param {type:\"number\"}\n",
        "import similarity, pandas as pd, numpy as np\n",
        "\n",
        "# Define a custom style function\n",
        "def highlight_below_standard(value):\n",
        "    if value < custom_threshold:\n",
        "        return 'background-color: red'\n",
        "    elif value >= custom_threshold and value < custom_threshold:\n",
        "        return 'background-color: yellow'\n",
        "    else:\n",
        "        return 'background-color: green'\n",
        "\n",
        "df_response = pd.read_excel(evaluation_data, sheet_name='response')# Read the Excel file into a pandas DataFrame based on sheet\n",
        "df_system = pd.read_excel(evaluation_data, sheet_name='system')# Read the Excel file into a pandas DataFrame based on sheet\n",
        "\n",
        "\n",
        "feature_names1 = ['Study {} Justification of Paper 1 Software Requirement Engineering'.format(variable_data),\n",
        "                 'Study {} Relevancy of Paper 1 Software Requirement Engineering'.format(variable_data),\n",
        "                 'Study {} Inclusion of Paper 1 Software Requirement Engineering'.format(variable_data)]\n",
        "feature_names2 = ['Study {} Justification of Paper 2 Software Quality Assurance'.format(variable_data),\n",
        "                 'Study {} Relevancy of Paper 2 Software Quality Assurance'.format(variable_data),\n",
        "                 'Study {} Inclusion of Paper 2 Software Quality Assurance'.format(variable_data)]\n",
        "feature_names3 = ['Study {} Justification of Paper 3 Network-based Computing'.format(variable_data),\n",
        "                 'Study {} Relevancy of Paper 3 Network-based Computing'.format(variable_data),\n",
        "                 'Study {} Inclusion of Paper 3 Network-based Computing'.format(variable_data)]\n",
        "feature_names4 = ['Study {} Justification of Paper 4 Visual Image Computation'.format(variable_data),\n",
        "                 'Study {} Relevancy of Paper 4 Visual Image Computation'.format(variable_data),\n",
        "                 'Study {} Inclusion of Paper 4 Visual Image Computation'.format(variable_data)]\n",
        "feature_names5 = ['Study {} Justification of Paper 5 Visual Image Computation'.format(variable_data),\n",
        "                 'Study {} Relevancy of Paper 5 Visual Image Computation'.format(variable_data),\n",
        "                 'Study {} Inclusion of Paper 5 Visual Image Computation'.format(variable_data)]\n",
        "\n",
        "data1_paper1 = [df_system[name].values.tolist() for name in feature_names1]\n",
        "data2_paper1 = [df_response[name].values.tolist() for name in feature_names1]\n",
        "data1_paper2 = [df_system[name].values.tolist() for name in feature_names2]\n",
        "data2_paper2 = [df_response[name].values.tolist() for name in feature_names2]\n",
        "data1_paper3 = [df_system[name].values.tolist() for name in feature_names3]\n",
        "data2_paper3 = [df_response[name].values.tolist() for name in feature_names3]\n",
        "data1_paper4 = [df_system[name].values.tolist() for name in feature_names4]\n",
        "data2_paper4 = [df_response[name].values.tolist() for name in feature_names4]\n",
        "data1_paper5 = [df_system[name].values.tolist() for name in feature_names5]\n",
        "data2_paper5 = [df_response[name].values.tolist() for name in feature_names5]\n",
        "\n",
        "data_paper1 = [similarity.get_string_similarity(num, angka) for angka in data2_paper1 for num in data1_paper1]\n",
        "data_paper2 = [similarity.get_string_similarity(num, angka) for angka in data2_paper2 for num in data1_paper2]\n",
        "data_paper3 = [similarity.get_string_similarity(num, angka) for angka in data2_paper3 for num in data1_paper3]\n",
        "data_paper4 = [similarity.get_string_similarity(num, angka) for angka in data2_paper4 for num in data1_paper4]\n",
        "data_paper5 = [similarity.get_string_similarity(num, angka) for angka in data2_paper5 for num in data1_paper5]\n",
        "\n",
        "print(\"\\nData System\")\n",
        "print(data1_paper1)\n",
        "print(\"\\nData Respondend\")\n",
        "print(data2_paper1)\n",
        "print(\"\\Matrix Evaluation\")\n",
        "print(data_paper1)\n",
        "\n",
        "print(\"Evaluation w/ more than one feature\")\n",
        "\n",
        "df_sim = pd.DataFrame([data_paper1, data_paper2, data_paper3, data_paper4, data_paper5])\n",
        "df_sim.columns = [\"respondent {}\".format(id+1) for id, num in enumerate(df_sim.columns)]\n",
        "df_sim.index = [\"article {}\".format(id+1) for id, num in enumerate(df_sim.index)]\n",
        "df_sim\n",
        "\n",
        "# if condition_data == \"default\":\n",
        "#   print(df_sim)\n",
        "\n",
        "# if condition_data == \"maximum\":\n",
        "#   print(df_sim.T.max())\n",
        "\n",
        "# if condition_data == \"threshold\":\n",
        "#   #Apply the style function to the DataFrame\n",
        "#   styled_df = df_sim.style.applymap(highlight_below_standard, subset=['Score'])\n",
        "#   styled_df = df_sim.style.applymap(highlight_below_standard)\n",
        "#   print(styled_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "cellView": "form",
        "id": "BE055on67K-J",
        "outputId": "a392eb96-c852-4227-fc71-de3f52b798b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data System\n",
            "[['This section provides an overview of the primary basis of reference for the study and explains how it builds upon previous research. The authors mention that three preliminary studies form the primary basis of reference in this study to contain the stages of making research methodology to seek new dependencies among the SRS documents. They then explain how their proposed method builds upon previous research by explicitly explaining each abstraction of previous studies revealed. The continuation framework proposed in a previous study is used as a basis for identifying dependencies among requirements, and Natural Language Processing is used to extract dependency relations.'], [9], ['Yes']]\n",
            "\n",
            "Data Respondend\n",
            "[['abstract has fulfilled scientific principles', 'clearly and concisely contains an abstract component', 'correct', 'Study Abstract is very good and applicable', 'It is concise and shows high clarity and accuracy, indicating clarity of results', 'applicable ', 'After read the abstract I know some important points, like what requirement the use, steps of the research and how the research would be developed.', 'no', 'It is better to provide a clear drawback and its solution.'], [8, 9, 10, 9, 9, 9, 9, 7, 7], ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']]\n",
            "\\Matrix Evaluation\n",
            "[0.6843077609857403, 0.4451403386685224, 0.5082413758822945, 0.4448685770062051, 0.5793650793650794, 0.39285714285714285, 0.39990858878120955, 0.44973544973544977, 0.6507936507936508]\n",
            "Evaluation w/ more than one feature\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           respondent 1  respondent 2  respondent 3  respondent 4  \\\n",
              "article 1      0.684308      0.445140      0.508241      0.444869   \n",
              "article 2      0.779274      0.445308      0.509080      0.416483   \n",
              "article 3      0.693781      0.445402      0.576218      0.429993   \n",
              "article 4      0.618989      0.445229      0.508683      0.412472   \n",
              "article 5      0.630218      0.445712      0.444432      0.426374   \n",
              "\n",
              "           respondent 5  respondent 6  respondent 7  respondent 8  \\\n",
              "article 1      0.579365      0.392857      0.399909      0.449735   \n",
              "article 2      0.580247      0.393298      0.391533      0.449735   \n",
              "article 3      0.580247      0.393298      0.436632      0.449735   \n",
              "article 4      0.580247      0.393298      0.405376      0.449821   \n",
              "article 5      0.456349      0.392857      0.411614      0.449909   \n",
              "\n",
              "           respondent 9  \n",
              "article 1      0.650794  \n",
              "article 2      0.650794  \n",
              "article 3      0.650794  \n",
              "article 4      0.540195  \n",
              "article 5      0.540723  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e1a4475-8110-4287-9f05-3663db1c3060\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>respondent 1</th>\n",
              "      <th>respondent 2</th>\n",
              "      <th>respondent 3</th>\n",
              "      <th>respondent 4</th>\n",
              "      <th>respondent 5</th>\n",
              "      <th>respondent 6</th>\n",
              "      <th>respondent 7</th>\n",
              "      <th>respondent 8</th>\n",
              "      <th>respondent 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>article 1</th>\n",
              "      <td>0.684308</td>\n",
              "      <td>0.445140</td>\n",
              "      <td>0.508241</td>\n",
              "      <td>0.444869</td>\n",
              "      <td>0.579365</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.399909</td>\n",
              "      <td>0.449735</td>\n",
              "      <td>0.650794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 2</th>\n",
              "      <td>0.779274</td>\n",
              "      <td>0.445308</td>\n",
              "      <td>0.509080</td>\n",
              "      <td>0.416483</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.391533</td>\n",
              "      <td>0.449735</td>\n",
              "      <td>0.650794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 3</th>\n",
              "      <td>0.693781</td>\n",
              "      <td>0.445402</td>\n",
              "      <td>0.576218</td>\n",
              "      <td>0.429993</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.436632</td>\n",
              "      <td>0.449735</td>\n",
              "      <td>0.650794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 4</th>\n",
              "      <td>0.618989</td>\n",
              "      <td>0.445229</td>\n",
              "      <td>0.508683</td>\n",
              "      <td>0.412472</td>\n",
              "      <td>0.580247</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.405376</td>\n",
              "      <td>0.449821</td>\n",
              "      <td>0.540195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>article 5</th>\n",
              "      <td>0.630218</td>\n",
              "      <td>0.445712</td>\n",
              "      <td>0.444432</td>\n",
              "      <td>0.426374</td>\n",
              "      <td>0.456349</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.411614</td>\n",
              "      <td>0.449909</td>\n",
              "      <td>0.540723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e1a4475-8110-4287-9f05-3663db1c3060')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e1a4475-8110-4287-9f05-3663db1c3060 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e1a4475-8110-4287-9f05-3663db1c3060');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_response.to_json('data_response.json', orient='records')\n",
        "print(\"data_response JSON file saved successfully.\")\n",
        "\n",
        "df_system.to_json('data_system.json', orient='records')\n",
        "print(\"data_system JSON file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v017FDmKR1pY",
        "outputId": "d6bac86f-8a5a-4ced-81c9-1aaffd48affc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_response JSON file saved successfully.\n",
            "data_system JSON file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Another Thunnel Scenario"
      ],
      "metadata": {
        "id": "P2Mlprm-0WQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scenario1_streamlit.py\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "def main():\n",
        "    st.title('Final Project: Scenario 1')\n",
        "\n",
        "    # Sidebar configuration\n",
        "    st.sidebar.title('Configuration')\n",
        "    config_path = st.sidebar.text_input('Config Path', 'config_scenario1.json')\n",
        "    huggingface_token_param = st.sidebar.text_input('Huggingface Token Param', '')\n",
        "    openapi_token_param = st.sidebar.text_input('OpenAPI Token Param', '')\n",
        "\n",
        "    # PDF file selection\n",
        "    st.header('PDF File Selection')\n",
        "    pdf_param = st.file_uploader('Upload PDF file', type=['pdf'])\n",
        "    if pdf_param is not None:\n",
        "        doc_reader = PdfReader(pdf_param)\n",
        "        raw_text = ''\n",
        "        for i, page in enumerate(doc_reader.pages):\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                raw_text += text\n",
        "        st.text(raw_text[:100])\n",
        "\n",
        "    # Text Splitting parameters\n",
        "    st.header('Text Splitting Parameters')\n",
        "    chunk_size_param = st.slider('Chunk Size', min_value=100, max_value=10000, value=1000)\n",
        "    chunk_overlap_param = st.slider('Chunk Overlap', min_value=0, max_value=1000, value=200)\n",
        "\n",
        "    # Query parameters\n",
        "    st.header('Query Parameters')\n",
        "    queries = []\n",
        "    for i in range(9):\n",
        "        query = st.text_input(f'Query {i}', '')\n",
        "        queries.append(query)\n",
        "\n",
        "    # Run button\n",
        "    if st.button('Run'):\n",
        "        config = {\n",
        "            'pdf_param': pdf_param,\n",
        "            'huggingface_token_param': huggingface_token_param,\n",
        "            'openapi_token_param': openapi_token_param,\n",
        "            'save_param': 'output.json',\n",
        "            'chunk_size_param': chunk_size_param,\n",
        "            'chunk_overlap_param': chunk_overlap_param,\n",
        "            'queries': queries\n",
        "        }\n",
        "\n",
        "        # Location of the pdf file/files\n",
        "        if pdf_param is not None:\n",
        "            doc_reader = PdfReader(pdf_param)\n",
        "            raw_text = ''\n",
        "            for i, page in enumerate(doc_reader.pages):\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    raw_text += text\n",
        "\n",
        "            # Splitting up the text into smaller chunks for indexing\n",
        "            text_splitter = CharacterTextSplitter(\n",
        "                separator=\"\\n\",\n",
        "                chunk_size=chunk_size_param,\n",
        "                chunk_overlap=chunk_overlap_param,\n",
        "                length_function=len,\n",
        "            )\n",
        "            texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "            # Download embeddings from OpenAI\n",
        "            if huggingface_token_param:\n",
        "                os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_token_param\n",
        "                embeddings = HuggingFaceEmbeddings()\n",
        "            elif openapi_token_param:\n",
        "                os.environ[\"OPENAI_API_KEY\"] = openapi_token_param\n",
        "                embeddings = OpenAIEmbeddings()\n",
        "\n",
        "            docsearch = embeddings.from_texts(texts)  # Indexing process without FAISS\n",
        "            responses = []\n",
        "\n",
        "            for query in queries:\n",
        "                docs = docsearch.similarity_search(query)\n",
        "                response = chain.run(input_documents=docs, question=query)\n",
        "                responses.append(response)\n",
        "                st.text(response)\n",
        "\n",
        "            # Sample string\n",
        "            data = {\n",
        "                \"author\": responses[0][0],\n",
        "                \"title\": responses[0][1],\n",
        "                \"Theoretical/ Conceptual Framework\": responses[0][2],\n",
        "                \"Research Question(s)/ Hypotheses\": responses[0][3],\n",
        "                \"methodology\": responses[0][4],\n",
        "                \"Analysis & Results study\": responses[0][5],\n",
        "                \"conclusion\": responses[0][6],\n",
        "                \"Implications for Future research\": responses[0][7],\n",
        "                \"Implication for practice\": responses[0][8],\n",
        "            }\n",
        "\n",
        "            # Save the JSON object to a file\n",
        "            with open('output.json', 'w') as f:\n",
        "                json.dump(data, f)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "BW0sGZJP0VIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scenario2_streamlit.py\n",
        "import os, json, torch, streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI, HuggingFaceHub\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "def main():\n",
        "    st.title('Final Project: Scenario 2')\n",
        "\n",
        "    # Sidebar configuration\n",
        "    st.sidebar.title('Configuration')\n",
        "    config_path = st.sidebar.text_input('Config Path', 'config_scenario2.json')\n",
        "    huggingface_token_param = st.sidebar.text_input('Huggingface Token Param', '')\n",
        "    openapi_token_param = st.sidebar.text_input('OpenAPI Token Param', '')\n",
        "\n",
        "    # Data directory selection\n",
        "    st.header('Data Directory Selection')\n",
        "    data_dir_param = st.text_input('Data Directory Path', '')\n",
        "\n",
        "    # Run button\n",
        "    if st.button('Run'):\n",
        "        config = {\n",
        "            'data_dir_param': data_dir_param,\n",
        "            'save_param': 'output.json',\n",
        "            'repo_id_param': '',\n",
        "            'huggingface_token_param': huggingface_token_param,\n",
        "            'openapi_token_param': openapi_token_param,\n",
        "            'openapi_model_param': '',\n",
        "            'chain_type_param': '',\n",
        "            'device_param': '',\n",
        "            'temperature_param': '',\n",
        "            'max_length_param': '',\n",
        "            'pad_token_id_param': '',\n",
        "            'top_p_param': '',\n",
        "            'repetation_penalty_param': '',\n",
        "            'chunk_size_param': '',\n",
        "            'chunk_overlap_param': '',\n",
        "            'huggingface_active': True if huggingface_token_param else False,\n",
        "            'openapi_active': True if openapi_token_param else False,\n",
        "            'queries': []\n",
        "        }\n",
        "\n",
        "        if data_dir_param:\n",
        "            data_list = os.listdir(data_dir_param)\n",
        "            for num in data_list:\n",
        "                reader = PdfReader(os.path.join(data_dir_param, num))\n",
        "                raw_text = ''\n",
        "                for i, page in enumerate(reader.pages):\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        raw_text += text\n",
        "\n",
        "                text_splitter = CharacterTextSplitter(\n",
        "                    separator=\"\\n\",\n",
        "                    chunk_size=config['chunk_size_param'],\n",
        "                    chunk_overlap=config['chunk_overlap_param'],\n",
        "                    length_function=len,\n",
        "                )\n",
        "                texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "                if config['huggingface_active']:\n",
        "                    os.environ['HUGGINGFACEHUB_API_TOKEN'] = config['huggingface_token_param']\n",
        "                    embeddings = HuggingFaceEmbeddings()\n",
        "                    llm_hf = HuggingFaceHub(\n",
        "                        repo_id=config['repo_id_param'],\n",
        "                        model_kwargs={\n",
        "                            'temperature': config['temperature_param'],\n",
        "                            'max_length': config['max_length_param'],\n",
        "                            'pad_token_id': config['pad_token_id_param'],\n",
        "                            'top_p': config['top_p_param'],\n",
        "                            'device': config['device_param'],\n",
        "                            'repetition_penalty': config['repetation_penalty_param']\n",
        "                        }\n",
        "                    )\n",
        "                    chain = load_qa_chain(llm=llm_hf, chain_type=config['chain_type_param'])\n",
        "                elif config['openapi_active']:\n",
        "                    os.environ[\"OPENAI_API_KEY\"] = config['openapi_token_param']\n",
        "                    embeddings = OpenAIEmbeddings()\n",
        "                    chain = load_qa_chain(llm=OpenAI(model=config['openapi_model_param']), chain_type=config['chain_type_param'])\n",
        "\n",
        "                docsearch = FAISS.from_texts(texts, embeddings)\n",
        "                responses = []\n",
        "\n",
        "                for query in config['queries']:\n",
        "                    docs = docsearch.similarity_search(query)\n",
        "                    response = chain.run(input_documents=docs, question=query)\n",
        "                    responses.append(response)\n",
        "\n",
        "                data = {\n",
        "                    \"author\": responses[0][0],\n",
        "                    \"title\": responses[0][1],\n",
        "                    \"Theoretical/ Conceptual Framework\": responses[0][2],\n",
        "                    \"Research Question(s)/ Hypotheses\": responses[0][3],\n",
        "                    \"methodology\": responses[0][4],\n",
        "                    \"Analysis & Results study\": responses[0][5],\n",
        "                    \"conclusion\": responses[0][6],\n",
        "                    \"Implications for Future research\": responses[0][7],\n",
        "                    \"Implication for practice\": responses[0][8]\n",
        "                }\n",
        "\n",
        "                with open('output.json', 'w') as f:\n",
        "                    json.dump(data, f)\n",
        "\n",
        "                st.success('Processing complete! Output saved to output.json')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "p89_sJwLWMX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !npm install localtunnel\n",
        "# !pip install -q ngrok streamlit\n",
        "# !python --version\n",
        "# !streamlit run scenario1.py & npx localtunnel --p 8501"
      ],
      "metadata": {
        "id": "8LnGA6spdjnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !streamlit run main.py & npx localtunnel --p 8501"
      ],
      "metadata": {
        "id": "xd7bAiaq0bi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}